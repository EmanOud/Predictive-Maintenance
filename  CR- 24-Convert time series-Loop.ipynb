{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Libraries\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd # to import csv and for data manipulation\n",
    "import matplotlib.pyplot as plt # to plot graph\n",
    "import seaborn as sns # for intractve graphs\n",
    "import numpy as np # for linear algebra\n",
    "import datetime # to dela with date and time\n",
    "%matplotlib inline\n",
    "from sklearn.preprocessing import StandardScaler # for preprocessing the data\n",
    "from sklearn.ensemble import RandomForestClassifier # Random forest classifier\n",
    "from sklearn.tree import DecisionTreeClassifier # for Decision Tree classifier\n",
    "from sklearn.svm import SVC # for SVM classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split # to split the data\n",
    "from sklearn.model_selection import KFold # For cross vbalidation\n",
    "from sklearn.model_selection import GridSearchCV # for tunnig hyper parameter it will use all combination of given parameters\n",
    "from sklearn.model_selection import RandomizedSearchCV # same for tunning hyper parameter but will use random combinations of parameters\n",
    "from sklearn.metrics import confusion_matrix,recall_score,precision_recall_curve,auc,roc_curve,roc_auc_score,classification_report\n",
    "import pandas  as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np \n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "telemetry = pd.read_csv('PdM_telemetry.csv')\n",
    "errors = pd.read_csv('PdM_errors.csv')\n",
    "maint = pd.read_csv('PdM_maint.csv')\n",
    "failures = pd.read_csv('PdM_failures.csv')\n",
    "machines = pd.read_csv('PdM_machines.csv')\n",
    "\n",
    "#format datetime field which comes in as string in telemetry\n",
    "telemetry['datetime'] = pd.to_datetime(telemetry['datetime'], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "#Print total number of records in telemetry\n",
    "#print('Total number of telemetry records: {}'.format(len(telemetry.index)))\n",
    "\n",
    "# format datetime field which comes in as string in Errors\n",
    "errors['datetime'] = pd.to_datetime(errors['datetime'], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "errors['errorID'] = errors['errorID'].astype('category')\n",
    "#Print total number of records in Errors\n",
    "#print('Total number of error records: {}'.format(len(errors.index)))\n",
    "\n",
    "# format datetime field which comes in as string in Maint\n",
    "maint['datetime'] = pd.to_datetime(maint['datetime'], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "maint['comp'] = maint['comp'].astype('category')\n",
    "#print('Total number of maintenance records: {}'.format(len(maint.index)))\n",
    "\n",
    "#Machine details\n",
    "machines['model'] = machines['model'].astype('category')\n",
    "#print('Total number of machines: {}'.format(len(machines.index)))\n",
    "\n",
    "# format datetime field which comes in as string\n",
    "failures['datetime'] = pd.to_datetime(failures['datetime'], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "failures['failure'] = failures['failure'].astype('category')\n",
    "#print('Total number of failures: {}'.format(len(failures.index)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define interval\n",
    "Hours='24H'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>machineID</th>\n",
       "      <th>datetime</th>\n",
       "      <th>voltmean_24h</th>\n",
       "      <th>rotatemean_24h</th>\n",
       "      <th>pressuremean_24h</th>\n",
       "      <th>vibrationmean_24h</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-01-02</td>\n",
       "      <td>167.576533</td>\n",
       "      <td>440.515328</td>\n",
       "      <td>98.522345</td>\n",
       "      <td>40.049623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-01-03</td>\n",
       "      <td>169.795758</td>\n",
       "      <td>446.832666</td>\n",
       "      <td>98.454608</td>\n",
       "      <td>39.271645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-01-04</td>\n",
       "      <td>171.862244</td>\n",
       "      <td>459.204742</td>\n",
       "      <td>97.998233</td>\n",
       "      <td>48.074091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-01-05</td>\n",
       "      <td>174.792428</td>\n",
       "      <td>448.743201</td>\n",
       "      <td>101.452266</td>\n",
       "      <td>52.190268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-01-06</td>\n",
       "      <td>171.018408</td>\n",
       "      <td>454.822750</td>\n",
       "      <td>102.363114</td>\n",
       "      <td>43.330311</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   machineID   datetime  voltmean_24h  rotatemean_24h  pressuremean_24h  \\\n",
       "0          1 2015-01-02    167.576533      440.515328         98.522345   \n",
       "1          1 2015-01-03    169.795758      446.832666         98.454608   \n",
       "2          1 2015-01-04    171.862244      459.204742         97.998233   \n",
       "3          1 2015-01-05    174.792428      448.743201        101.452266   \n",
       "4          1 2015-01-06    171.018408      454.822750        102.363114   \n",
       "\n",
       "   vibrationmean_24h  \n",
       "0          40.049623  \n",
       "1          39.271645  \n",
       "2          48.074091  \n",
       "3          52.190268  \n",
       "4          43.330311  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate mean values for telemetry features\n",
    "temp = []\n",
    "fields = ['volt', 'rotate', 'pressure', 'vibration']\n",
    "for col in fields:\n",
    "    temp.append(pd.pivot_table(telemetry,\n",
    "                               index='datetime',\n",
    "                               columns='machineID',\n",
    "                               values=col).resample(Hours, closed='left', label='right').mean().unstack())\n",
    "telemetry_mean_24h = pd.concat(temp, axis=1)\n",
    "telemetry_mean_24h.columns = [i + 'mean_24h' for i in fields]\n",
    "telemetry_mean_24h.reset_index(inplace=True)\n",
    "\n",
    "# repeat for standard deviation\n",
    "temp = []\n",
    "for col in fields:\n",
    "    temp.append(pd.pivot_table(telemetry,\n",
    "                               index='datetime',\n",
    "                               columns='machineID',\n",
    "                               values=col).resample(Hours, closed='left', label='right').std().unstack())\n",
    "telemetry_sd_24h = pd.concat(temp, axis=1)\n",
    "telemetry_sd_24h.columns = [i + 'sd_24h' for i in fields]\n",
    "telemetry_sd_24h.reset_index(inplace=True)\n",
    "\n",
    "telemetry_mean_24h.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge columns of feature sets created earlier\n",
    "telemetry_feat = pd.concat([telemetry_mean_24h,\n",
    "                            telemetry_sd_24h.ix[:, 2:6]], axis=1).dropna()\n",
    "#telemetry_feat.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a column for each error type\n",
    "error_count = pd.get_dummies(errors)\n",
    "error_count.columns = ['datetime', 'machineID', 'error1', 'error2', 'error3', 'error4', 'error5']\n",
    "\n",
    "# combine errors for a given machine in a given hour\n",
    "error_count = error_count.groupby(['machineID', 'datetime']).sum().reset_index()\n",
    "#error_count.head()\n",
    "\n",
    "#Merge 2 datasets, telemetry and errors\n",
    "error_count = telemetry[['datetime', 'machineID']].merge(error_count, on=['machineID', 'datetime'], how='left').fillna(0.0)\n",
    "#error_count.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = []\n",
    "fields = ['error%d' % i for i in range(1,6)]\n",
    "for col in fields:\n",
    "    temp.append(pd.pivot_table(error_count,\n",
    "                               index='datetime',\n",
    "                               columns='machineID',\n",
    "                               values=col).resample(Hours,closed='left',label='right').sum().unstack())\n",
    "error_count = pd.concat(temp, axis=1)\n",
    "error_count.columns = [i + 'count' for i in fields]\n",
    "error_count.reset_index(inplace=True)\n",
    "error_count = error_count.dropna()\n",
    "#error_count.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labeled_features = final_feat.merge(failures, on=['datetime', 'machineID'], how='left')\n",
    "# fill backward up to 24h \n",
    "#labeled_features = labeled_features.bfill(axis=1, limit=5)\n",
    "#labeled_features = labeled_features.fillna('none')\n",
    "#labeled_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a column for each failure type\n",
    "failures_count = pd.get_dummies(failures)\n",
    "failures_count.columns = ['datetime', 'machineID', 'Fail-comp1', 'Fail-comp2', 'Fail-comp3', 'Fail-comp4']\n",
    "\n",
    "# combine faiures for a given machine in a given hour\n",
    "failures_count = failures_count.groupby(['machineID', 'datetime']).sum().reset_index()\n",
    "failures_count = telemetry[['datetime', 'machineID']].merge(failures_count, on=['machineID', 'datetime'], how='left').fillna(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = []\n",
    "fields = ['Fail-comp1', 'Fail-comp2', 'Fail-comp3', 'Fail-comp4']\n",
    "for col in fields:\n",
    "    temp.append(pd.pivot_table(failures_count,\n",
    "                               index='datetime',\n",
    "                               columns='machineID',\n",
    "                               values=col).resample(Hours,closed='left',\n",
    "                               label='right').sum().unstack())\n",
    "failures_count = pd.concat(temp, axis=1)\n",
    "failures_count.columns = ['Fail-comp1', 'Fail-comp2', 'Fail-comp3', 'Fail-comp4']\n",
    "failures_count.reset_index(inplace=True)\n",
    "failures_count = failures_count.dropna()\n",
    "#failures_count.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# create a column for each error type\n",
    "comp_rep = pd.get_dummies(maint)\n",
    "comp_rep.columns = ['datetime', 'machineID','comp1_rep', 'comp2_rep', 'comp3_rep', 'comp4_rep']\n",
    "\n",
    "# combine repairs for a given machine in a given hour\n",
    "comp_rep = comp_rep.groupby(['machineID', 'datetime']).sum().reset_index()\n",
    "\n",
    "# add timepoints where no components were replaced\n",
    "comp_rep = telemetry[['datetime', 'machineID']].merge(comp_rep,\n",
    "                                                      on=['datetime', 'machineID'],\n",
    "                                                      how='outer').fillna(0).sort_values(by=['machineID', 'datetime'])\n",
    "\n",
    "components = ['comp1_rep', 'comp2_rep', 'comp3_rep', 'comp4_rep']\n",
    "for comp in components:\n",
    "    # convert indicator to most recent date of component change\n",
    "    comp_rep.loc[comp_rep[comp] < 1, comp] = None\n",
    "    comp_rep.loc[-comp_rep[comp].isnull(), comp] = comp_rep.loc[-comp_rep[comp].isnull(), 'datetime']\n",
    "    \n",
    "    # forward-fill the most-recent date of component change\n",
    "    comp_rep[comp] = comp_rep[comp].fillna(method='ffill')\n",
    "    \n",
    "# remove dates in 2014 (may have NaN or future component change dates)    \n",
    "comp_rep = comp_rep.loc[comp_rep['datetime'] > pd.to_datetime('2015-01-01')]\n",
    "\n",
    "for comp in components:\n",
    "    comp_rep[comp] = pd.to_datetime(comp_rep[comp], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "    comp_rep[comp] = (comp_rep['datetime'] - comp_rep[comp]).apply(lambda x: x / pd.Timedelta(days=1))\n",
    "#comp_rep.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merging the dataset\n",
    "final_feat = telemetry_feat.merge(machines, on=['machineID'], how='left')\n",
    "final_feat =final_feat.merge(error_count, on=['datetime', 'machineID'], how='left')\n",
    "final_feat = final_feat.merge(comp_rep, on=['datetime', 'machineID'], how='left')\n",
    "final_feat = final_feat.merge(failures_count, on=['datetime', 'machineID'], how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 100 Seperated DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting dataset per machineID\n",
    "final_featM = {}\n",
    "for MID in range(1,101):\n",
    "    final_featM[MID] = pd.DataFrame(final_feat.loc[final_feat[\"machineID\"]==MID])\n",
    "    \n",
    "#for MID in range(1,101):\n",
    "     #print(final_featM[MID].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#New coloums created\n",
    "for MID in range(1,101):\n",
    "    final_featM[MID]['Errors'] = final_featM[MID][\"error1count\"]+ final_featM[MID][\"error4count\"] +final_featM[MID][\"error2count\"]+ final_featM[MID][\"error3count\"]+ final_featM[MID][\"error5count\"]\n",
    "    final_featM[MID]['Sum of Comp-Fail']=final_featM[MID]['Fail-comp1']+final_featM[MID]['Fail-comp2'] + final_featM[MID]['Fail-comp3'] +final_featM[MID]['Fail-comp4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for MID in range(1,101):\n",
    "    #Count time until to failure coloumn\n",
    "    x=-1\n",
    "    Count = []\n",
    "    count = [] \n",
    "    DifferenceBetweenFailures = {}\n",
    "    for Mcomp1 in  final_featM[MID]['Sum of Comp-Fail']: \n",
    "            if ((Mcomp1 == 0)): \n",
    "                count.append(x+1) \n",
    "                x=x+1\n",
    "            elif ((Mcomp1 >= 1)): \n",
    "                count.append(x+1)\n",
    "                Count.append (x)\n",
    "                x=-1   \n",
    "    Count.append (x)\n",
    "    final_featM[MID][\"CountLifetotal\"] = count\n",
    "    DifferenceBetweenFailures[MID] = pd.DataFrame(Count)\n",
    " #Flip to calculate RUL\n",
    "    RULComp1 = [] \n",
    "    x=-1\n",
    "    R=0\n",
    "    Y=0\n",
    "    Y2=0\n",
    "    for T in  final_featM[MID][\"CountLifetotal\"]: \n",
    "           if (x<(len(DifferenceBetweenFailures[MID])-1)):\n",
    "                if (T > 0): \n",
    "                    RULComp1.append((DifferenceBetweenFailures[MID].loc[x,0])-R) \n",
    "                    R=R+1\n",
    "                elif ((T == 0)): \n",
    "                    x=x+1\n",
    "                    RULComp1.append((DifferenceBetweenFailures[MID].loc[x,0]))\n",
    "                    R=0\n",
    "           elif (x == len(DifferenceBetweenFailures[MID])):\n",
    "                RULComp1.append((DifferenceBetweenFailures[MID].loc[x,0])-Y)\n",
    "                Y=Y+1\n",
    "                x=len(dfComp1)\n",
    "           else:\n",
    "                x=len(DifferenceBetweenFailures[MID])-1\n",
    "                RULComp1.append((DifferenceBetweenFailures[MID].loc[x,0])-Y2)\n",
    "                Y2=Y2+1\n",
    "\n",
    "    final_featM[MID]['Sum of Comp-RUL'] = RULComp1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for MID in range(1,101):\n",
    "     #print(final_featM[MID].head())\n",
    "\n",
    "#Saving as CSV files\n",
    "#for MID in range(1,101):\n",
    "     #final_featM[MID].to_csv('24hrAv{}.csv'.format(MID))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_featLR={}\n",
    "for MID in range(1,101):\n",
    "     final_featM[MID]['Failure']=final_featM[MID]['Sum of Comp-Fail']\n",
    "     final_featLR[MID]=final_featM[MID].drop(['Fail-comp1','Fail-comp2','Fail-comp3',\n",
    "                                       'Fail-comp4','machineID','Sum of Comp-RUL',\n",
    "                                              'Sum of Comp-Fail','error1count','error2count',\n",
    "                                              'error3count','error4count','error5count'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp=[]\n",
    "for MID in range(1,50):\n",
    "    temp.append(final_featLR[MID])\n",
    "data=pd.concat(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.0\n",
       "1    0.0\n",
       "2    0.0\n",
       "3    0.0\n",
       "4    1.0\n",
       "Name: Failure, dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Failure'].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    17590\n",
       "1.0      319\n",
       "2.0       25\n",
       "Name: Failure, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Failure'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #0 or 1\n",
    "    ColoumSum=[]\n",
    "    for T in data['Failure']:\n",
    "        if ((T == 0)): \n",
    "            ColoumSum.append(0) \n",
    "        elif ((T >= 1)): \n",
    "            ColoumSum.append(1)\n",
    "\n",
    "    data['Failure'] = ColoumSum\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    17590\n",
       "1      344\n",
       "Name: Failure, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Failure'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percentage of no failures is 98.08185569309691\n",
      "percentage of failures 1.9181443069030892\n"
     ]
    }
   ],
   "source": [
    "count_no_sub = len(data[data['Failure']==0])\n",
    "count_sub = len(data[data['Failure']==1])\n",
    "pct_of_no_sub = count_no_sub/(count_no_sub+count_sub)\n",
    "print(\"percentage of no failures is\", pct_of_no_sub*100)\n",
    "pct_of_sub = count_sub/(count_no_sub+count_sub)\n",
    "print(\"percentage of failures\", pct_of_sub*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count time until to failure coloumn\n",
    "x=1\n",
    "ColoumSum=[]\n",
    "for Mcomp1 in  data['Failure']: \n",
    "        if ((Mcomp1 == 0)): \n",
    "            ColoumSum.append(x) \n",
    "        elif ((Mcomp1 >= 1)): \n",
    "            ColoumSum.append(x+1)\n",
    "            x=x+1\n",
    "\n",
    "data[\"ColoumSum\"] = ColoumSum\n",
    "#print(data[\"ColoumSum\"]) \n",
    "feat1=[]\n",
    "for T in range(1,x+1):\n",
    "    df=data.loc[data[\"ColoumSum\"]== T]\n",
    "    #AVG\n",
    "    df['AvVolt2'] = df.iloc[:,2].rolling(window=4,min_periods = 0).mean().fillna(df['voltmean_24h'])\n",
    "    df['AvRotate2'] =df.iloc[:,3].rolling(window=4,min_periods = 0).mean().fillna(df['rotatemean_24h'])\n",
    "    df['AvPressure2'] = df.iloc[:,4].rolling(window=4,min_periods = 0).mean().fillna(df['pressuremean_24h'])\n",
    "    df['AvVibration2'] = df.iloc[:,5].rolling(window=4,min_periods = 0).mean().fillna(df['vibrationmean_24h'])\n",
    "    #STD\n",
    "    df['STDVolt2'] = df.iloc[:,2].rolling(window=4,min_periods = 0).std().fillna(df['voltsd_24h'])\n",
    "    df['STDRotate2'] = df.iloc[:,3].rolling(window=4,min_periods = 0).std().fillna(df['rotatesd_24h'])\n",
    "    df['STDPressure2'] = df.iloc[:,4].rolling(window=4,min_periods = 0).std().fillna(df['pressuresd_24h'])\n",
    "    df['STDVibration2'] = df.iloc[:,5].rolling(window=4,min_periods = 0).std().fillna(df['vibrationsd_24h'])\n",
    "\n",
    "    feat1.append(df)\n",
    "    final_feat1=pd.concat(feat1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data= final_feat1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.drop(['ColoumSum'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 17934 entries, 0 to 17933\n",
      "Data columns (total 26 columns):\n",
      "datetime             17934 non-null datetime64[ns]\n",
      "voltmean_24h         17934 non-null float64\n",
      "rotatemean_24h       17934 non-null float64\n",
      "pressuremean_24h     17934 non-null float64\n",
      "vibrationmean_24h    17934 non-null float64\n",
      "voltsd_24h           17934 non-null float64\n",
      "rotatesd_24h         17934 non-null float64\n",
      "pressuresd_24h       17934 non-null float64\n",
      "vibrationsd_24h      17934 non-null float64\n",
      "model                17934 non-null category\n",
      "age                  17934 non-null int64\n",
      "comp1_rep            17885 non-null float64\n",
      "comp2_rep            17885 non-null float64\n",
      "comp3_rep            17885 non-null float64\n",
      "comp4_rep            17885 non-null float64\n",
      "Errors               17934 non-null float64\n",
      "CountLifetotal       17934 non-null int64\n",
      "Failure              17934 non-null int64\n",
      "AvVolt2              17934 non-null float64\n",
      "AvRotate2            17934 non-null float64\n",
      "AvPressure2          17934 non-null float64\n",
      "AvVibration2         17934 non-null float64\n",
      "STDVolt2             17934 non-null float64\n",
      "STDRotate2           17934 non-null float64\n",
      "STDPressure2         17934 non-null float64\n",
      "STDVibration2        17934 non-null float64\n",
      "dtypes: category(1), datetime64[ns](1), float64(21), int64(3)\n",
      "memory usage: 3.6 MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.ffill(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Changing to dummy coloumns\n",
    "cat_vars=['model']\n",
    "for var in cat_vars:\n",
    "    cat_list='var'+'_'+var\n",
    "    cat_list = pd.get_dummies(data[var], prefix=var)\n",
    "    data1=data.join(cat_list)\n",
    "    data=data1\n",
    "cat_vars=['model']\n",
    "data_vars=data.columns.values.tolist()\n",
    "to_keep=[i for i in data_vars if i not in cat_vars]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['datetime', 'voltmean_24h', 'rotatemean_24h', 'pressuremean_24h',\n",
       "       'vibrationmean_24h', 'voltsd_24h', 'rotatesd_24h',\n",
       "       'pressuresd_24h', 'vibrationsd_24h', 'age', 'comp1_rep',\n",
       "       'comp2_rep', 'comp3_rep', 'comp4_rep', 'Errors', 'CountLifetotal',\n",
       "       'Failure', 'AvVolt2', 'AvRotate2', 'AvPressure2', 'AvVibration2',\n",
       "       'STDVolt2', 'STDRotate2', 'STDPressure2', 'STDVibration2',\n",
       "       'model_model1', 'model_model2', 'model_model3', 'model_model4'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=data[to_keep]\n",
    "data.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0\n",
      "1    0\n",
      "2    0\n",
      "3    0\n",
      "4    1\n",
      "Name: Failure, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(data['Failure'].head())\n",
    "data2=data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shift Starts Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Failure+2  Failure+1  Failure  Failure-1  Failure-2  Failure-3  \\\n",
      "2            0.0        0.0        0        0.0        1.0        0.0   \n",
      "3            0.0        0.0        0        1.0        0.0        0.0   \n",
      "4            0.0        0.0        1        0.0        0.0        0.0   \n",
      "5            0.0        1.0        0        0.0        0.0        0.0   \n",
      "6            1.0        0.0        0        0.0        0.0        0.0   \n",
      "...          ...        ...      ...        ...        ...        ...   \n",
      "17922        0.0        0.0        0        0.0        0.0        0.0   \n",
      "17923        0.0        0.0        0        0.0        0.0        0.0   \n",
      "17924        0.0        0.0        0        0.0        0.0        1.0   \n",
      "17925        0.0        0.0        0        0.0        1.0        0.0   \n",
      "17926        0.0        0.0        0        1.0        0.0        0.0   \n",
      "\n",
      "       Failure-4  Failure-5  Failure-6  Failure-7  \n",
      "2            0.0        0.0        0.0        0.0  \n",
      "3            0.0        0.0        0.0        0.0  \n",
      "4            0.0        0.0        0.0        0.0  \n",
      "5            0.0        0.0        0.0        0.0  \n",
      "6            0.0        0.0        0.0        0.0  \n",
      "...          ...        ...        ...        ...  \n",
      "17922        0.0        1.0        0.0        0.0  \n",
      "17923        1.0        0.0        0.0        0.0  \n",
      "17924        0.0        0.0        0.0        0.0  \n",
      "17925        0.0        0.0        0.0        0.0  \n",
      "17926        0.0        0.0        0.0        0.0  \n",
      "\n",
      "[17925 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    " \n",
    "def series_to_supervised(data, n_in, n_out, dropnan=True):\n",
    "    \n",
    "#Frame a time series as a supervised learning dataset.\n",
    "#Arguments:\n",
    "#data: Sequence of observations as a list or NumPy array.\n",
    "#n_in: Number of lag observations as input (X).\n",
    "#n_out: Number of observations as output (y).\n",
    "#dropnan: Boolean whether or not to drop rows with NaN values.\n",
    "#Returns:\n",
    "#Pandas DataFrame of series framed for supervised learning.\n",
    "\n",
    "    #n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        if i == 0:\n",
    "            names += [('Failure')]\n",
    "        elif i == 1:\n",
    "            names += [('Failure+1')]\n",
    "        elif i == 2:\n",
    "            names += [('Failure+2')]\n",
    "        #elif i == 3:\n",
    "         #   names += [('Failure+3')]\n",
    "        #elif i == 4:\n",
    "         #   names += [('Failure+4')]\n",
    "        #elif i == 5:\n",
    "         #   names += [('Failure+5')]\n",
    "        #elif i == 6:\n",
    "         #   names += [('Failure+6')]\n",
    "        #elif i == 7:\n",
    "         #   names += [('Failure+7')]\n",
    "        \n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('Failure')]\n",
    "        elif i == 1:\n",
    "            names += [('Failure-1')]\n",
    "        elif i == 2:\n",
    "            names += [('Failure-2')]\n",
    "        elif i == 3:\n",
    "            names += [('Failure-3')]\n",
    "        elif i == 4:\n",
    "            names += [('Failure-4')]\n",
    "        elif i == 5:\n",
    "            names += [('Failure-5')]\n",
    "        elif i == 6:\n",
    "            names += [('Failure-6')]\n",
    "        elif i == 7:\n",
    "            names += [('Failure-7')]\n",
    "            \n",
    "    # put it all together\n",
    "    agg = concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg\n",
    " \n",
    "values = data2['Failure']\n",
    "data = series_to_supervised(values, 2,8)\n",
    "data=data.fillna(0)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2['Failure-1']=data['Failure-1']\n",
    "data2['Failure-2']=data['Failure-2']\n",
    "data2['Failure-3']=data['Failure-3']\n",
    "data2['Failure-4']=data['Failure-4']\n",
    "data2['Failure-5']=data['Failure-5']\n",
    "data2['Failure-6']=data['Failure-6']\n",
    "data2['Failure-7']=data['Failure-7']\n",
    "\n",
    "data2['Failure+1']=data['Failure+1']\n",
    "data2['Failure+2']=data['Failure+2']\n",
    "#data2['Failure+3']=data['Failure+3']\n",
    "#data2['Failure+4']=data['Failure+4']\n",
    "#data2['Failure+5']=data['Failure+5']\n",
    "#data2['Failure+6']=data['Failure+6']\n",
    "#data2['Failure+7']=data['Failure+7']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Errors+7  Errors+6  Errors+5  Errors+4  Errors+3  Errors+2  Errors+1\n",
      "0           0.0       0.0       0.0       0.0       0.0       0.0       0.0\n",
      "1           0.0       0.0       0.0       0.0       0.0       0.0       0.0\n",
      "2           0.0       0.0       0.0       0.0       0.0       0.0       0.0\n",
      "3           0.0       0.0       0.0       0.0       0.0       0.0       2.0\n",
      "4           0.0       0.0       0.0       0.0       0.0       2.0       1.0\n",
      "...         ...       ...       ...       ...       ...       ...       ...\n",
      "17929       0.0       0.0       0.0       0.0       1.0       0.0       0.0\n",
      "17930       0.0       0.0       0.0       1.0       0.0       0.0       0.0\n",
      "17931       0.0       0.0       1.0       0.0       0.0       0.0       0.0\n",
      "17932       0.0       1.0       0.0       0.0       0.0       0.0       0.0\n",
      "17933       1.0       0.0       0.0       0.0       0.0       0.0       0.0\n",
      "\n",
      "[17934 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    " \n",
    "def series_to_supervised(data, n_in, n_out, dropnan=False):\n",
    "    df = DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        #names += [('Errors+1')]\n",
    "        if i == 0:\n",
    "            names += [('Errors')]\n",
    "        elif i == 1:\n",
    "            names += [('Errors+1')]\n",
    "        elif i == 2:\n",
    "            names += [('Errors+2')]\n",
    "        elif i == 3:\n",
    "            names += [('Errors+3')] \n",
    "        elif i == 4:\n",
    "            names += [('Errors+4')]\n",
    "        elif i == 5:\n",
    "            names += [('Errors+5')]\n",
    "        elif i == 6:\n",
    "            names += [('Errors+6')]\n",
    "        elif i == 7:\n",
    "            names += [('Errors+7')]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    #for i in range(0, n_out):\n",
    "        #cols.append(df.shift(-i))\n",
    "        #if i == 0:\n",
    "         #   names += [('Errors')]\n",
    "        #elif i == 1:\n",
    "         #   names += [('Errors-1')]\n",
    "        #elif i == 2:\n",
    "         #   names += [('Errors-2')]\n",
    "        #elif i == 3:\n",
    "         #   names += [('Errors-3')] \n",
    "        #elif i == 4:\n",
    "         #   names += [('Errors-4')]\n",
    "        #elif i == 5:\n",
    "         #   names += [('Errors-5')]\n",
    "        #elif i == 6:\n",
    "         #   names += [('Errors-6')]\n",
    "        #elif i == 7:\n",
    "         #   names += [('Errors-7')]\n",
    "\n",
    "    # put it all together\n",
    "    agg = concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg\n",
    " \n",
    "values = data2['Errors']\n",
    "data = series_to_supervised(values, 7, 1)\n",
    "data=data.fillna(0)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2['Errors+1']=data['Errors+1']\n",
    "data2['Errors+2']=data['Errors+2']\n",
    "data2['Errors+3']=data['Errors+3']\n",
    "data2['Errors+4']=data['Errors+4']\n",
    "data2['Errors+5']=data['Errors+5']\n",
    "data2['Errors+6']=data['Errors+6']\n",
    "data2['Errors+7']=data['Errors+7']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2=data2.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sh=-1\n",
    "#Delete Failure coloumns to shift failure up\n",
    "#data2['Failure']=data2['Failure'].shift(Sh).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    0\n",
       "2    0\n",
       "3    0\n",
       "4    1\n",
       "Name: Failure, dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2['Failure'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 17934 entries, 0 to 17933\n",
      "Data columns (total 45 columns):\n",
      "datetime             17934 non-null datetime64[ns]\n",
      "voltmean_24h         17934 non-null float64\n",
      "rotatemean_24h       17934 non-null float64\n",
      "pressuremean_24h     17934 non-null float64\n",
      "vibrationmean_24h    17934 non-null float64\n",
      "voltsd_24h           17934 non-null float64\n",
      "rotatesd_24h         17934 non-null float64\n",
      "pressuresd_24h       17934 non-null float64\n",
      "vibrationsd_24h      17934 non-null float64\n",
      "age                  17934 non-null int64\n",
      "comp1_rep            17934 non-null float64\n",
      "comp2_rep            17934 non-null float64\n",
      "comp3_rep            17934 non-null float64\n",
      "comp4_rep            17934 non-null float64\n",
      "Errors               17934 non-null float64\n",
      "CountLifetotal       17934 non-null int64\n",
      "Failure              17934 non-null int64\n",
      "AvVolt2              17934 non-null float64\n",
      "AvRotate2            17934 non-null float64\n",
      "AvPressure2          17934 non-null float64\n",
      "AvVibration2         17934 non-null float64\n",
      "STDVolt2             17934 non-null float64\n",
      "STDRotate2           17934 non-null float64\n",
      "STDPressure2         17934 non-null float64\n",
      "STDVibration2        17934 non-null float64\n",
      "model_model1         17934 non-null uint8\n",
      "model_model2         17934 non-null uint8\n",
      "model_model3         17934 non-null uint8\n",
      "model_model4         17934 non-null uint8\n",
      "Failure-1            17934 non-null float64\n",
      "Failure-2            17934 non-null float64\n",
      "Failure-3            17934 non-null float64\n",
      "Failure-4            17934 non-null float64\n",
      "Failure-5            17934 non-null float64\n",
      "Failure-6            17934 non-null float64\n",
      "Failure-7            17934 non-null float64\n",
      "Failure+1            17934 non-null float64\n",
      "Failure+2            17934 non-null float64\n",
      "Errors+1             17934 non-null float64\n",
      "Errors+2             17934 non-null float64\n",
      "Errors+3             17934 non-null float64\n",
      "Errors+4             17934 non-null float64\n",
      "Errors+5             17934 non-null float64\n",
      "Errors+6             17934 non-null float64\n",
      "Errors+7             17934 non-null float64\n",
      "dtypes: datetime64[ns](1), float64(37), int64(3), uint8(4)\n",
      "memory usage: 6.4 MB\n"
     ]
    }
   ],
   "source": [
    "data2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2.to_csv('checkna.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "AccScore=[]\n",
    "# make test and training splits\n",
    "threshold_dates = [[pd.to_datetime('2015-07-31 01:00:00'), pd.to_datetime('2015-08-01 01:00:00')],\n",
    "                   [pd.to_datetime('2015-08-31 01:00:00'), pd.to_datetime('2015-09-01 01:00:00')],\n",
    "                   [pd.to_datetime('2015-09-30 01:00:00'), pd.to_datetime('2015-10-01 01:00:00')]]\n",
    "\n",
    "\n",
    "for last_train_date, first_test_date in threshold_dates:\n",
    "    # split out training and test data\n",
    "    y_train = data2.loc[data2['datetime'] <pd.to_datetime('2015-08-30 01:00:00'), 'Failure-1']\n",
    "    X_train= pd.get_dummies(data2.loc[data2['datetime'] < pd.to_datetime('2015-08-30 01:00:00')].drop(['datetime',      \n",
    "                                                                                                        'Failure','Failure-1','Failure-2','Failure-3','Failure-4','Failure-5','Failure-6','Failure-7',\n",
    "                                                                                                      'Errors+1','Errors+2','Errors+3','Errors+4','Errors+5','Errors+6','Errors+7'], 1))\n",
    "    X_test = pd.get_dummies(data2.loc[data2['datetime'] >  pd.to_datetime('2015-09-01 01:00:00')].drop(['datetime',\n",
    "                                                                                                       'Failure','Failure-1','Failure-2','Failure-3','Failure-4','Failure-5','Failure-6','Failure-7',\n",
    "                                                                                                       'Errors+1','Errors+2','Errors+3','Errors+4','Errors+5','Errors+6','Errors+7'] ,1))\n",
    "    y_test = data2.loc[data2['datetime'] >  pd.to_datetime('2015-09-01 01:00:00'), 'Failure-1']\n",
    "    \n",
    "    # Feature Scaling\n",
    "    #scaler = StandardScaler()\n",
    "    #train_X = scaler.fit_transform(train_X)\n",
    "    #test_X = scaler.transform(test_X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.875, 0.27586206896551724, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.9973452795752448, 0.9858967977434876, 0.9809191969470715, 0.9809191969470715, 0.9809191969470715, 0.9809191969470715, 0.9805873568939771]\n",
      "[0.9245283018867924, 0.42953020134228187, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "RecallScore=[]\n",
    "AccuracyScore=[]\n",
    "f1Score=[]\n",
    "for i in range (1,8):\n",
    "    # make test and training splits\n",
    "    threshold_dates = [[pd.to_datetime('2015-07-31 01:00:00'), pd.to_datetime('2015-08-01 01:00:00')],\n",
    "                       [pd.to_datetime('2015-08-31 01:00:00'), pd.to_datetime('2015-09-01 01:00:00')],\n",
    "                       [pd.to_datetime('2015-09-30 01:00:00'), pd.to_datetime('2015-10-01 01:00:00')]]\n",
    "\n",
    "\n",
    "    for last_train_date, first_test_date in threshold_dates:\n",
    "        # split out training and test data\n",
    "        y_train = data2.loc[data2['datetime'] <pd.to_datetime('2015-08-30 01:00:00'), 'Failure-{}'.format(i)]\n",
    "        X_train= pd.get_dummies(data2.loc[data2['datetime'] < pd.to_datetime('2015-08-30 01:00:00')].drop(['datetime',      \n",
    "                                                                                                            'Failure','Failure-1','Failure-2','Failure-3','Failure-4','Failure-5','Failure-6','Failure-7',\n",
    "                                                                                                          'Errors+1','Errors+2','Errors+3','Errors+4','Errors+5','Errors+6','Errors+7'], 1))\n",
    "        X_test = pd.get_dummies(data2.loc[data2['datetime'] >  pd.to_datetime('2015-09-01 01:00:00')].drop(['datetime',\n",
    "                                                                                                           'Failure','Failure-1','Failure-2','Failure-3','Failure-4','Failure-5','Failure-6','Failure-7',\n",
    "                                                                                                           'Errors+1','Errors+2','Errors+3','Errors+4','Errors+5','Errors+6','Errors+7'] ,1))\n",
    "        y_test = data2.loc[data2['datetime'] >  pd.to_datetime('2015-09-01 01:00:00'), 'Failure-{}'.format(i)]\n",
    "\n",
    "    \n",
    "    \n",
    "    #logistic regression object \n",
    "    RF = RandomForestClassifier(n_estimators=100) \n",
    "\n",
    "    # train the model on train set \n",
    "    RF.fit(X_train, y_train.values.ravel()) \n",
    "\n",
    "    predictions = RF.predict(X_test) \n",
    "\n",
    "    # print classification report \n",
    "    #print(classification_report(y_test, predictions))\n",
    "    #Recall Value\n",
    "    # Use score method to get accuracy of model\n",
    "\n",
    "    Ascore = accuracy_score(y_test,predictions)\n",
    "    AccuracyScore.append(Ascore)\n",
    "    \n",
    "    Rscore = recall_score(y_test,predictions)\n",
    "    RecallScore.append(Rscore)\n",
    "    \n",
    "    f1score = f1_score(y_test,predictions)\n",
    "    f1Score.append(f1score)\n",
    "\n",
    "print (RecallScore)\n",
    "print(AccuracyScore)\n",
    "print(f1Score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.to_csv('tesdata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "X= data2.ix[:,data2.columns != \"Failure-1\"].drop(['datetime','Failure','Failure-2','Failure-3','Failure-4','Failure-5','Failure-6','Failure-7'],1)\n",
    "y=data2.ix[:,data2.columns==\"Failure-1\"]\n",
    "#print(cross_val_score(lr1, X, y, cv=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fit_time': array([0.38831139, 0.40737128, 0.43855643, 0.40930247, 0.4482522 ,\n",
      "       0.41630316, 0.43034482, 0.39389014, 0.4042604 , 0.41349006]), 'score_time': array([0.02326298, 0.02251935, 0.02226925, 0.01651716, 0.01765633,\n",
      "       0.01523972, 0.01541471, 0.02089   , 0.01892495, 0.01657677]), 'test_accuracy': array([0.98216276, 0.98439242, 0.98550725, 0.97658863, 0.96932515,\n",
      "       0.98717234, 0.98717234, 0.97992192, 0.98047964, 0.97936419]), 'test_precision': array([0.33333333, 0.5       , 0.52941176, 0.6875    , 0.46153846,\n",
      "       0.4       , 0.91666667, 0.45454545, 0.5       , 0.42857143]), 'test_recall': array([0.18518519, 0.17857143, 0.33333333, 0.22916667, 0.11111111,\n",
      "       0.19047619, 0.33333333, 0.14285714, 0.08571429, 0.17142857]), 'test_f1_score': array([0.23809524, 0.26315789, 0.40909091, 0.34375   , 0.17910448,\n",
      "       0.25806452, 0.48888889, 0.2173913 , 0.14634146, 0.24489796])}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import KFold\n",
    "scoring = {'accuracy' : make_scorer(accuracy_score), \n",
    "           'precision' : make_scorer(precision_score),\n",
    "           'recall' : make_scorer(recall_score), \n",
    "           'f1_score' : make_scorer(f1_score)}\n",
    "\n",
    "kfold = KFold(n_splits=10)\n",
    "\n",
    "results = cross_validate (estimator=lr, X=X,y=y,cv=kfold, scoring=scoring)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      1.00      0.99      5915\n",
      "         1.0       0.44      0.17      0.25       112\n",
      "\n",
      "    accuracy                           0.98      6027\n",
      "   macro avg       0.71      0.58      0.62      6027\n",
      "weighted avg       0.97      0.98      0.98      6027\n",
      "\n",
      "[[5891   24]\n",
      " [  93   19]]\n"
     ]
    }
   ],
   "source": [
    "y_pred_new_threshold = (lr.predict_proba(X_test)[:,1]>=0.5).astype(int)\n",
    "\n",
    "print(classification_report(y_test, y_pred_new_threshold))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix = confusion_matrix(y_test,y_pred_new_threshold)\n",
    "print(confusion_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      1.00      0.99      5915\n",
      "         1.0       0.49      0.21      0.30       112\n",
      "\n",
      "    accuracy                           0.98      6027\n",
      "   macro avg       0.74      0.61      0.64      6027\n",
      "weighted avg       0.98      0.98      0.98      6027\n",
      "\n",
      "[[5890   25]\n",
      " [  88   24]]\n"
     ]
    }
   ],
   "source": [
    "# logistic regression object \n",
    "lr = LogisticRegression() \n",
    "  \n",
    "# train the model on train set \n",
    "lr.fit(X_train, y_train.values.ravel()) \n",
    "  \n",
    "predictions = lr.predict(X_test) \n",
    "  \n",
    "# print classification report \n",
    "print(classification_report(y_test, predictions))\n",
    "\n",
    "#from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix = confusion_matrix(y_test,predictions)\n",
    "print(confusion_matrix)\n",
    "\n",
    " #get importance\n",
    "#importance = lr.coef_[0]\n",
    "#summarize feature importance\n",
    "#for i,v in enumerate(importance):\n",
    " #   print('Feature: %0d, Score: %.5f' % (i,v))\n",
    "#plot feature importance\n",
    "#plt.bar([x for x in range(len(importance))], importance)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# logistic regression object \n",
    "lr = RandomForestRegressor() \n",
    "  \n",
    "# train the model on train set \n",
    "lr.fit(X_train, y_train.values.ravel()) \n",
    "  \n",
    "predictions = lr.predict(X_test) \n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 0.00476854156296665\n",
      "Mean Squared Error: 0.0023617388418782147\n",
      "Root Mean Squared Error: 0.04859772465741801\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the Algorithm\n",
    "from sklearn import metrics\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, predictions ))  \n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, predictions ))  \n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, predictions )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 0.01874896299983408\n",
      "Mean Squared Error: 0.01874896299983408\n",
      "Root Mean Squared Error: 0.13692685273471408\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the Algorithm\n",
    "from sklearn import metrics\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, predictions ))  \n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, predictions ))  \n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, predictions )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "#Recall Value\n",
    "# Use score method to get accuracy of model\n",
    "score = recall_score(y_test,predictions)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "#f1_score\n",
    "from sklearn.metrics import f1_score\n",
    "# Use score method to get accuracy of model\n",
    "score = f1_score(y_test,predictions)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9814169570267132\n"
     ]
    }
   ],
   "source": [
    " from sklearn.metrics import accuracy_score\n",
    "# Use score method to get accuracy of model\n",
    "score = accuracy_score(y_test,predictions)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature: 0, Score: 0.16325\n",
      "Feature: 1, Score: -0.01990\n",
      "Feature: 2, Score: 0.07455\n",
      "Feature: 3, Score: 0.16875\n",
      "Feature: 4, Score: -0.01608\n",
      "Feature: 5, Score: -0.03465\n",
      "Feature: 6, Score: -0.02334\n",
      "Feature: 7, Score: -0.01365\n",
      "Feature: 8, Score: 0.05100\n",
      "Feature: 9, Score: 0.02927\n",
      "Feature: 10, Score: 0.02850\n",
      "Feature: 11, Score: 0.02981\n",
      "Feature: 12, Score: 0.01816\n",
      "Feature: 13, Score: 0.02528\n",
      "Feature: 14, Score: 0.00109\n",
      "Feature: 15, Score: 0.00243\n",
      "Feature: 16, Score: 0.00243\n",
      "Feature: 17, Score: 0.00511\n",
      "Feature: 18, Score: 0.00828\n",
      "Feature: 19, Score: -0.03960\n",
      "Feature: 20, Score: 0.08464\n",
      "Feature: 21, Score: 0.14757\n",
      "Feature: 22, Score: 0.01208\n",
      "Feature: 23, Score: -0.02886\n",
      "Feature: 24, Score: -0.08495\n",
      "Feature: 25, Score: 0.00054\n",
      "Feature: 26, Score: 0.00647\n",
      "Feature: 27, Score: -0.06036\n",
      "Feature: 28, Score: -0.05460\n",
      "Feature: 29, Score: -0.01877\n",
      "Feature: 30, Score: -0.01649\n",
      "Feature: 31, Score: -0.06046\n",
      "Feature: 32, Score: -0.03749\n",
      "Feature: 33, Score: 0.01843\n",
      "Feature: 34, Score: 0.00059\n",
      "Feature: 35, Score: 0.01320\n",
      "Feature: 36, Score: 0.00070\n",
      "Feature: 37, Score: -0.00575\n",
      "Feature: 38, Score: -0.00883\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# logistic regression for feature importance\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "from matplotlib import pyplot\n",
    "# get importance\n",
    "importance = lr.coef_[0]\n",
    "# summarize feature importance\n",
    "for i,v in enumerate(importance):\n",
    "\tprint('Feature: %0d, Score: %.5f' % (i,v))\n",
    "# plot feature importance\n",
    "pyplot.bar([x for x in range(len(importance))], importance)\n",
    "pyplot.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before OverSampling, counts of label '1': 231\n",
      "Before OverSampling, counts of label '0': 11578 \n",
      "\n",
      "After OverSampling, the shape of train_X: (23156, 39)\n",
      "After OverSampling, the shape of train_y: (23156,) \n",
      "\n",
      "After OverSampling, counts of label '1': 11578\n",
      "After OverSampling, counts of label '0': 11578\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.92      0.96      5915\n",
      "         1.0       0.18      0.94      0.30       112\n",
      "\n",
      "    accuracy                           0.92      6027\n",
      "   macro avg       0.59      0.93      0.63      6027\n",
      "weighted avg       0.98      0.92      0.94      6027\n",
      "\n",
      "[[5431  484]\n",
      " [   7  105]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Before OverSampling, counts of label '1': {}\".format(sum(y_train == 1))) \n",
    "print(\"Before OverSampling, counts of label '0': {} \\n\".format(sum(y_train == 0))) \n",
    "  \n",
    "# import SMOTE module from imblearn library \n",
    "# pip install imblearn (if you don't have imblearn in your system) \n",
    "from imblearn.over_sampling import SMOTE \n",
    "sm = SMOTE(random_state = 2) \n",
    "X_train_res, y_train_res = sm.fit_sample(X_train, y_train.values.ravel()) \n",
    "  \n",
    "print('After OverSampling, the shape of train_X: {}'.format(X_train_res.shape)) \n",
    "print('After OverSampling, the shape of train_y: {} \\n'.format(y_train_res.shape)) \n",
    "  \n",
    "print(\"After OverSampling, counts of label '1': {}\".format(sum(y_train_res == 1))) \n",
    "print(\"After OverSampling, counts of label '0': {}\".format(sum(y_train_res == 0))) \n",
    "\n",
    "lr1 = LogisticRegression() \n",
    "lr1.fit(X_train_res, y_train_res.ravel()) \n",
    "predictions = lr1.predict(X_test) \n",
    "  \n",
    "# print classification report \n",
    "print(classification_report(y_test, predictions))\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix = confusion_matrix(y_test,predictions)\n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#UnderSampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Undersampling, counts of label '1': 231\n",
      "Before Undersampling, counts of label '0': 11578 \n",
      "\n",
      "After Undersampling, the shape of train_X: (462, 39)\n",
      "After Undersampling, the shape of train_y: (462,) \n",
      "\n",
      "After Undersampling, counts of label '1': 231\n",
      "After Undersampling, counts of label '0': 231\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.76      0.86      5915\n",
      "         1.0       0.07      0.92      0.13       112\n",
      "\n",
      "    accuracy                           0.76      6027\n",
      "   macro avg       0.53      0.84      0.50      6027\n",
      "weighted avg       0.98      0.76      0.85      6027\n",
      "\n",
      "[[4506 1409]\n",
      " [   9  103]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Before Undersampling, counts of label '1': {}\".format(sum(y_train == 1))) \n",
    "print(\"Before Undersampling, counts of label '0': {} \\n\".format(sum(y_train == 0))) \n",
    "  \n",
    "# apply near miss \n",
    "from imblearn.under_sampling import NearMiss \n",
    "nr = NearMiss() \n",
    "  \n",
    "X_train_miss, y_train_miss = nr.fit_sample(X_train, y_train.values.ravel()) \n",
    "  \n",
    "print('After Undersampling, the shape of train_X: {}'.format(X_train_miss.shape)) \n",
    "print('After Undersampling, the shape of train_y: {} \\n'.format(y_train_miss.shape)) \n",
    "  \n",
    "print(\"After Undersampling, counts of label '1': {}\".format(sum(y_train_miss == 1))) \n",
    "print(\"After Undersampling, counts of label '0': {}\".format(sum(y_train_miss == 0)))\n",
    "\n",
    "# train the model on train set \n",
    "lr2 = LogisticRegression() \n",
    "lr2.fit(X_train_miss, y_train_miss.ravel()) \n",
    "predictions = lr2.predict(X_test) \n",
    "  \n",
    "# print classification report \n",
    "print(classification_report(y_test, predictions))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix = confusion_matrix(y_test,predictions)\n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 6027 entries, 243 to 17933\n",
      "Data columns (total 41 columns):\n",
      "voltmean_24h         6027 non-null float64\n",
      "rotatemean_24h       6027 non-null float64\n",
      "pressuremean_24h     6027 non-null float64\n",
      "vibrationmean_24h    6027 non-null float64\n",
      "voltsd_24h           6027 non-null float64\n",
      "rotatesd_24h         6027 non-null float64\n",
      "pressuresd_24h       6027 non-null float64\n",
      "vibrationsd_24h      6027 non-null float64\n",
      "age                  6027 non-null int64\n",
      "comp1_rep            6027 non-null float64\n",
      "comp2_rep            6027 non-null float64\n",
      "comp3_rep            6027 non-null float64\n",
      "comp4_rep            6027 non-null float64\n",
      "Errors               6027 non-null float64\n",
      "CountLifetotal       6027 non-null int64\n",
      "AvVolt2              6027 non-null float64\n",
      "AvRotate2            6027 non-null float64\n",
      "AvPressure2          6027 non-null float64\n",
      "AvVibration2         6027 non-null float64\n",
      "STDVolt2             6027 non-null float64\n",
      "STDRotate2           6027 non-null float64\n",
      "STDPressure2         6027 non-null float64\n",
      "STDVibration2        6027 non-null float64\n",
      "model_model1         6027 non-null uint8\n",
      "model_model2         6027 non-null uint8\n",
      "model_model3         6027 non-null uint8\n",
      "model_model4         6027 non-null uint8\n",
      "Failure-1            6027 non-null float64\n",
      "Failure-2            6027 non-null float64\n",
      "Failure-3            6027 non-null float64\n",
      "Failure-4            6027 non-null float64\n",
      "Failure-5            6027 non-null float64\n",
      "Failure-6            6027 non-null float64\n",
      "Failure-7            6027 non-null float64\n",
      "Errors-1             6027 non-null float64\n",
      "Errors-2             6027 non-null float64\n",
      "Errors-3             6027 non-null float64\n",
      "Errors-4             6027 non-null float64\n",
      "Errors-5             6027 non-null float64\n",
      "Errors-6             6027 non-null float64\n",
      "Errors-7             6027 non-null float64\n",
      "dtypes: float64(35), int64(2), uint8(4)\n",
      "memory usage: 1.8 MB\n"
     ]
    }
   ],
   "source": [
    "X_test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00      5915\n",
      "         1.0       0.98      0.88      0.92       112\n",
      "\n",
      "    accuracy                           1.00      6027\n",
      "   macro avg       0.99      0.94      0.96      6027\n",
      "weighted avg       1.00      1.00      1.00      6027\n",
      "\n",
      "[[5913    2]\n",
      " [  14   98]]\n",
      "Feature: 0, Score: 0.08917\n",
      "Feature: 1, Score: 0.05598\n",
      "Feature: 2, Score: 0.04396\n",
      "Feature: 3, Score: 0.05689\n",
      "Feature: 4, Score: 0.01363\n",
      "Feature: 5, Score: 0.01248\n",
      "Feature: 6, Score: 0.01452\n",
      "Feature: 7, Score: 0.01587\n",
      "Feature: 8, Score: 0.01416\n",
      "Feature: 9, Score: 0.03335\n",
      "Feature: 10, Score: 0.03633\n",
      "Feature: 11, Score: 0.02402\n",
      "Feature: 12, Score: 0.04135\n",
      "Feature: 13, Score: 0.33556\n",
      "Feature: 14, Score: 0.01772\n",
      "Feature: 15, Score: 0.02978\n",
      "Feature: 16, Score: 0.02013\n",
      "Feature: 17, Score: 0.03236\n",
      "Feature: 18, Score: 0.01493\n",
      "Feature: 19, Score: 0.02369\n",
      "Feature: 20, Score: 0.01868\n",
      "Feature: 21, Score: 0.02262\n",
      "Feature: 22, Score: 0.01457\n",
      "Feature: 23, Score: 0.00881\n",
      "Feature: 24, Score: 0.00209\n",
      "Feature: 25, Score: 0.00375\n",
      "Feature: 26, Score: 0.00236\n",
      "Feature: 27, Score: 0.00029\n",
      "Feature: 28, Score: 0.00013\n",
      "Feature: 29, Score: 0.00006\n",
      "Feature: 30, Score: 0.00019\n",
      "Feature: 31, Score: 0.00035\n",
      "Feature: 32, Score: 0.00000\n",
      "Feature: 33, Score: 0.00024\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "# logistic regression object \n",
    "lr = RandomForestClassifier(n_estimators=100) \n",
    "  \n",
    "# train the model on train set \n",
    "lr.fit(X_train, y_train.values.ravel()) \n",
    "  \n",
    "predictions = lr.predict(X_test) \n",
    "  \n",
    "# print classification report \n",
    "print(classification_report(y_test, predictions))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix = confusion_matrix(y_test,predictions)\n",
    "print(confusion_matrix)\n",
    "\n",
    " #get importance\n",
    "importance = lr.feature_importances_\n",
    "#summarize feature importance\n",
    "for i,v in enumerate(importance):\n",
    "    print('Feature: %0d, Score: %.5f' % (i,v))\n",
    "#plot feature importance\n",
    "plt.bar([x for x in range(len(importance))], importance)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before OverSampling, counts of label '1': 225\n",
      "Before OverSampling, counts of label '0': 11584 \n",
      "\n",
      "After OverSampling, the shape of train_X: (23168, 41)\n",
      "After OverSampling, the shape of train_y: (23168,) \n",
      "\n",
      "After OverSampling, counts of label '1': 11584\n",
      "After OverSampling, counts of label '0': 11584\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.99      0.99      5912\n",
      "         1.0       0.14      0.07      0.09       115\n",
      "\n",
      "    accuracy                           0.97      6027\n",
      "   macro avg       0.56      0.53      0.54      6027\n",
      "weighted avg       0.97      0.97      0.97      6027\n",
      "\n",
      "[[5863   49]\n",
      " [ 107    8]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Before OverSampling, counts of label '1': {}\".format(sum(y_train == 1))) \n",
    "print(\"Before OverSampling, counts of label '0': {} \\n\".format(sum(y_train == 0))) \n",
    "  \n",
    "# import SMOTE module from imblearn library \n",
    "# pip install imblearn (if you don't have imblearn in your system) \n",
    "from imblearn.over_sampling import SMOTE \n",
    "sm = SMOTE(random_state = 2) \n",
    "X_train_res, y_train_res = sm.fit_sample(X_train, y_train.values.ravel()) \n",
    "  \n",
    "print('After OverSampling, the shape of train_X: {}'.format(X_train_res.shape)) \n",
    "print('After OverSampling, the shape of train_y: {} \\n'.format(y_train_res.shape)) \n",
    "  \n",
    "print(\"After OverSampling, counts of label '1': {}\".format(sum(y_train_res == 1))) \n",
    "print(\"After OverSampling, counts of label '0': {}\".format(sum(y_train_res == 0))) \n",
    "\n",
    "lr1 = RandomForestClassifier(n_estimators=100)  \n",
    "lr1.fit(X_train_res, y_train_res.ravel()) \n",
    "predictions = lr1.predict(X_test) \n",
    "  \n",
    "# print classification report \n",
    "print(classification_report(y_test, predictions))\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix = confusion_matrix(y_test,predictions)\n",
    "print(confusion_matrix)\n",
    "\n",
    "# get importance\n",
    "#importance = lr1.feature_importances_\n",
    "# summarize feature importance\n",
    "#for i,v in enumerate(importance):\n",
    "    #print('Feature: %0d, Score: %.5f' % (i,v))\n",
    "# plot feature importance\n",
    "#pyplot.bar([x for x in range(len(importance))], importance)\n",
    "#pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Undersampling, counts of label '1': 230\n",
      "Before Undersampling, counts of label '0': 11579 \n",
      "\n",
      "After Undersampling, the shape of train_X: (460, 41)\n",
      "After Undersampling, the shape of train_y: (460,) \n",
      "\n",
      "After Undersampling, counts of label '1': 230\n",
      "After Undersampling, counts of label '0': 230\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.96      0.98      5915\n",
      "         1.0       0.31      1.00      0.47       112\n",
      "\n",
      "    accuracy                           0.96      6027\n",
      "   macro avg       0.65      0.98      0.72      6027\n",
      "weighted avg       0.99      0.96      0.97      6027\n",
      "\n",
      "[[5663  252]\n",
      " [   0  112]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Before Undersampling, counts of label '1': {}\".format(sum(y_train == 1))) \n",
    "print(\"Before Undersampling, counts of label '0': {} \\n\".format(sum(y_train == 0))) \n",
    "  \n",
    "# apply near miss \n",
    "from imblearn.under_sampling import NearMiss \n",
    "nr = NearMiss() \n",
    "  \n",
    "X_train_miss, y_train_miss = nr.fit_sample(X_train, y_train.values.ravel()) \n",
    "  \n",
    "print('After Undersampling, the shape of train_X: {}'.format(X_train_miss.shape)) \n",
    "print('After Undersampling, the shape of train_y: {} \\n'.format(y_train_miss.shape)) \n",
    "  \n",
    "print(\"After Undersampling, counts of label '1': {}\".format(sum(y_train_miss == 1))) \n",
    "print(\"After Undersampling, counts of label '0': {}\".format(sum(y_train_miss == 0)))\n",
    "\n",
    "# train the model on train set \n",
    "lr2 = RandomForestClassifier(n_estimators=100) \n",
    "lr2.fit(X_train_miss, y_train_miss.ravel()) \n",
    "predictions = lr2.predict(X_test) \n",
    "  \n",
    "# print classification report \n",
    "print(classification_report(y_test, predictions))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix = confusion_matrix(y_test,predictions)\n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00      5915\n",
      "         1.0       0.97      0.96      0.97       112\n",
      "\n",
      "    accuracy                           1.00      6027\n",
      "   macro avg       0.99      0.98      0.98      6027\n",
      "weighted avg       1.00      1.00      1.00      6027\n",
      "\n",
      "[[5912    3]\n",
      " [   4  108]]\n"
     ]
    }
   ],
   "source": [
    "# logistic regression object \n",
    "lr = GradientBoostingClassifier() \n",
    "  \n",
    "# train the model on train set \n",
    "lr.fit(X_train, y_train.values.ravel()) \n",
    "  \n",
    "predictions = lr.predict(X_test) \n",
    "  \n",
    "# print classification report \n",
    "print(classification_report(y_test, predictions))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix = confusion_matrix(y_test,predictions)\n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before OverSampling, counts of label '1': 231\n",
      "Before OverSampling, counts of label '0': 11578 \n",
      "\n",
      "After OverSampling, the shape of train_X: (23156, 39)\n",
      "After OverSampling, the shape of train_y: (23156,) \n",
      "\n",
      "After OverSampling, counts of label '1': 11578\n",
      "After OverSampling, counts of label '0': 11578\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00      5915\n",
      "         1.0       0.95      1.00      0.97       112\n",
      "\n",
      "    accuracy                           1.00      6027\n",
      "   macro avg       0.97      1.00      0.99      6027\n",
      "weighted avg       1.00      1.00      1.00      6027\n",
      "\n",
      "[[5909    6]\n",
      " [   0  112]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Before OverSampling, counts of label '1': {}\".format(sum(y_train == 1))) \n",
    "print(\"Before OverSampling, counts of label '0': {} \\n\".format(sum(y_train == 0))) \n",
    "  \n",
    "# import SMOTE module from imblearn library \n",
    "# pip install imblearn (if you don't have imblearn in your system) \n",
    "from imblearn.over_sampling import SMOTE \n",
    "sm = SMOTE(random_state = 2) \n",
    "X_train_res, y_train_res = sm.fit_sample(X_train, y_train.values.ravel()) \n",
    "  \n",
    "print('After OverSampling, the shape of train_X: {}'.format(X_train_res.shape)) \n",
    "print('After OverSampling, the shape of train_y: {} \\n'.format(y_train_res.shape)) \n",
    "  \n",
    "print(\"After OverSampling, counts of label '1': {}\".format(sum(y_train_res == 1))) \n",
    "print(\"After OverSampling, counts of label '0': {}\".format(sum(y_train_res == 0))) \n",
    "\n",
    "lr1 =  GradientBoostingClassifier() \n",
    "lr1.fit(X_train_res, y_train_res.ravel()) \n",
    "predictions = lr1.predict(X_test) \n",
    "  \n",
    "# print classification report \n",
    "print(classification_report(y_test, predictions))\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix = confusion_matrix(y_test,predictions)\n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Undersampling, counts of label '1': 231\n",
      "Before Undersampling, counts of label '0': 11578 \n",
      "\n",
      "After Undersampling, the shape of train_X: (462, 39)\n",
      "After Undersampling, the shape of train_y: (462,) \n",
      "\n",
      "After Undersampling, counts of label '1': 231\n",
      "After Undersampling, counts of label '0': 231\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.95      0.98      5915\n",
      "         1.0       0.29      1.00      0.45       112\n",
      "\n",
      "    accuracy                           0.95      6027\n",
      "   macro avg       0.64      0.98      0.71      6027\n",
      "weighted avg       0.99      0.95      0.97      6027\n",
      "\n",
      "[[5638  277]\n",
      " [   0  112]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Before Undersampling, counts of label '1': {}\".format(sum(y_train == 1))) \n",
    "print(\"Before Undersampling, counts of label '0': {} \\n\".format(sum(y_train == 0))) \n",
    "  \n",
    "# apply near miss \n",
    "from imblearn.under_sampling import NearMiss \n",
    "nr = NearMiss() \n",
    "  \n",
    "X_train_miss, y_train_miss = nr.fit_sample(X_train, y_train.values.ravel()) \n",
    "  \n",
    "print('After Undersampling, the shape of train_X: {}'.format(X_train_miss.shape)) \n",
    "print('After Undersampling, the shape of train_y: {} \\n'.format(y_train_miss.shape)) \n",
    "  \n",
    "print(\"After Undersampling, counts of label '1': {}\".format(sum(y_train_miss == 1))) \n",
    "print(\"After Undersampling, counts of label '0': {}\".format(sum(y_train_miss == 0)))\n",
    "\n",
    "# train the model on train set \n",
    "lr2 =  GradientBoostingClassifier() \n",
    "lr2.fit(X_train_miss, y_train_miss.ravel()) \n",
    "predictions = lr2.predict(X_test) \n",
    "  \n",
    "# print classification report \n",
    "print(classification_report(y_test, predictions))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix = confusion_matrix(y_test,predictions)\n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of binary and continuous targets",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-138-ec12b063097b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mconfusion_matrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36mconfusion_matrix\u001b[1;34m(y_true, y_pred, labels, sample_weight, normalize)\u001b[0m\n\u001b[0;32m    266\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m     \"\"\"\n\u001b[1;32m--> 268\u001b[1;33m     \u001b[0my_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    269\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"binary\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"multiclass\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    270\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%s is not supported\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0my_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     88\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m         raise ValueError(\"Classification metrics can't handle a mix of {0} \"\n\u001b[1;32m---> 90\u001b[1;33m                          \"and {1} targets\".format(type_true, type_pred))\n\u001b[0m\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m     \u001b[1;31m# We can't have more than one value on y_type => The set is no more needed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Classification metrics can't handle a mix of binary and continuous targets"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "# logistic regression object \n",
    "lr = GradientBoostingRegressor() \n",
    "  \n",
    "# train the model on train set \n",
    "lr.fit(X_train, y_train.values.ravel()) \n",
    "  \n",
    "predictions = lr.predict(X_test) \n",
    "  \n",
    "# print classification report \n",
    "#print(classification_report(y_test, predictions))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix = confusion_matrix(y_test,predictions)\n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before OverSampling, counts of label '1': 231\n",
      "Before OverSampling, counts of label '0': 11578 \n",
      "\n",
      "After OverSampling, the shape of train_X: (23156, 39)\n",
      "After OverSampling, the shape of train_y: (23156,) \n",
      "\n",
      "After OverSampling, counts of label '1': 11578\n",
      "After OverSampling, counts of label '0': 11578\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of binary and continuous targets",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-139-d6b894de0f7a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;31m# print classification report\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[0mconfusion_matrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36mclassification_report\u001b[1;34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict, zero_division)\u001b[0m\n\u001b[0;32m   1969\u001b[0m     \"\"\"\n\u001b[0;32m   1970\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1971\u001b[1;33m     \u001b[0my_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1972\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1973\u001b[0m     \u001b[0mlabels_given\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     88\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m         raise ValueError(\"Classification metrics can't handle a mix of {0} \"\n\u001b[1;32m---> 90\u001b[1;33m                          \"and {1} targets\".format(type_true, type_pred))\n\u001b[0m\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m     \u001b[1;31m# We can't have more than one value on y_type => The set is no more needed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Classification metrics can't handle a mix of binary and continuous targets"
     ]
    }
   ],
   "source": [
    "print(\"Before OverSampling, counts of label '1': {}\".format(sum(y_train == 1))) \n",
    "print(\"Before OverSampling, counts of label '0': {} \\n\".format(sum(y_train == 0))) \n",
    "  \n",
    "# import SMOTE module from imblearn library \n",
    "# pip install imblearn (if you don't have imblearn in your system) \n",
    "from imblearn.over_sampling import SMOTE \n",
    "sm = SMOTE(random_state = 2) \n",
    "X_train_res, y_train_res = sm.fit_sample(X_train, y_train.values.ravel()) \n",
    "  \n",
    "print('After OverSampling, the shape of train_X: {}'.format(X_train_res.shape)) \n",
    "print('After OverSampling, the shape of train_y: {} \\n'.format(y_train_res.shape)) \n",
    "  \n",
    "print(\"After OverSampling, counts of label '1': {}\".format(sum(y_train_res == 1))) \n",
    "print(\"After OverSampling, counts of label '0': {}\".format(sum(y_train_res == 0))) \n",
    "\n",
    "lr1 =  GradientBoostingRegressor() \n",
    "lr1.fit(X_train_res, y_train_res.ravel()) \n",
    "predictions = lr1.predict(X_test) \n",
    "  \n",
    "# print classification report \n",
    "print(classification_report(y_test, predictions))\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix = confusion_matrix(y_test,predictions)\n",
    "print(confusion_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
